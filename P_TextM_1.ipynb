{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7933fd05-298c-4988-810a-cfd669e5f48a",
   "metadata": {},
   "source": [
    "# * *PRÁCTICA FINAL SOTO ANÁLISIS DE SENTIMIENTOS* *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d681084-27d6-481c-800a-b883fb4a60aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import spacymoji\n",
    "import codecs\n",
    "import collections\n",
    "from spacymoji import Emoji\n",
    "from spacy import displacy\n",
    "import demoji \n",
    "  \n",
    "\n",
    "sys.path.append(os.path.realpath('../'))\n",
    "# from scripts.preprocess_data import Preprocess\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PATH_DATA = \"../\"\n",
    "DEV_FILE = \"dev/dev.tsv\"\n",
    "TRAIN_FILE = \"Corpus/train.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beef658e-41d3-41b8-b08b-453385c788a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuenta la frecuencia de aparicion  de cada tipo de entidades nombradas que aparecen en los tweets\n",
    "def entidades_nombradas(doc,nlp):\n",
    "    \n",
    "    labels = [e.label_ for e in doc.ents]\n",
    "    number = collections.Counter(labels)\n",
    "    print(\"Entity categories: \",number)\n",
    "    \n",
    "    #imprime las entidades nombradas que encuentra en el texto\n",
    "    #for sent in doc.sents:\n",
    "    #    displacy.render(nlp(sent.text),style='ent',jupyter=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91931435-a8eb-43ad-b54f-ca3bea69c3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#función donde podremos ver las entidades nombradas,las palabras más frecuentes y\n",
    "#tipo de palabras más frecuentes presentes en nuestros datos.\n",
    "\n",
    "def analisis_tweets(tweets):\n",
    "    \n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    \n",
    "    for pos,tweet in enumerate(tweets):\n",
    "        #con decode cambiamos los datos de binario a string\n",
    "        tweets[pos]=tweet.decode('UTF-8')\n",
    "    \n",
    "    \n",
    "    #pasamos de una lista de tweets un string de tweets\n",
    "    sentence = \"\"\n",
    "    for pos,tweet in enumerate(tweets):\n",
    "        sentence += str(tweet.lower()) + \".\"\n",
    "    \n",
    "   \n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    #obtenemos entidades nombradas que hay en el texto\n",
    "    entidades_nombradas(doc,nlp)\n",
    "\n",
    "    #obtenemos la frecuencia de tipos de palabras en el texto\n",
    "     # elimina stop words y signos de puntuación\n",
    "    tipo_words = [token.pos_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "    frecuencias = []\n",
    "    for w in tipo_words:\n",
    "        frecuencias.append(tipo_words.count(w))\n",
    "\n",
    "    pairList = list(zip(tipo_words, frecuencias))\n",
    "    \n",
    "    setPairList = set(pairList)\n",
    "\n",
    "    print(\"*********************************\")\n",
    "    unique_words = [word for (word, freq) in setPairList if freq > 50]\n",
    "    print(\"Tipo de palabras mas frecuentes: \", unique_words)\n",
    "    print(\"*********************************\")\n",
    "    \n",
    "    #obtenemos la frecuencia de las palabras en el texto\n",
    "    # elimina stop words y signos de puntuación\n",
    "    words = [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "    frecuencias = []\n",
    "    for w in words:\n",
    "        frecuencias.append(words.count(w))\n",
    "\n",
    "    pairList = list(zip(words, frecuencias))\n",
    "    \n",
    "    setPairList = set(pairList)\n",
    "\n",
    "    print(\"*********************************\")\n",
    "    unique_words = [word for (word, freq) in setPairList if freq > 30]\n",
    "    print(\"Palabras mas frecuentes: \", unique_words)\n",
    "    print(\"*********************************\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ddd27ca-cc4b-4f28-848a-839608b4741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminamos los emojis que aparen en los tweets\n",
    "def give_emoji_free_text(text):\n",
    "    #return emoji.get_emoji_regexp().sub(u'', text) # requiere que no sea string\n",
    "    text2 = text.encode('ascii', 'ignore').decode()\n",
    "    return text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e9431bf-6b7f-4b3d-92d0-34a15c6c5320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_processing(X):\n",
    "    \n",
    "    #lista de tweets limpios\n",
    "    tweets_clean = []\n",
    "    \n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "    for pos, sen in enumerate(X):\n",
    "        \n",
    "        tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', sen)  # remove hashtags + espacio at (hashtag con espacio)\n",
    "        tweet = re.sub('(#\\s+[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove hashtags at (hashtag)\n",
    "        tweet = re.sub('@', '', tweet)  # remove tweeted at (menciones)\n",
    "        #eliminamos emojis de los tweets\n",
    "        tweet = give_emoji_free_text(tweet)\n",
    "        \n",
    "        \n",
    "        \n",
    "        doc = nlp(tweet)\n",
    "\n",
    "        \n",
    "        #tokenizamos y hacemos pos y eliminamos urls\n",
    "        lista_tweet=[]\n",
    "        newSentence=''\n",
    "        for token in doc:\n",
    "            if (\n",
    "                #not token.is_punct\n",
    "                #not token.is_emoji\n",
    "                #and not token.like_num\n",
    "                #and not token.is_stop\n",
    "                not token.like_url\n",
    "                # and not token.is_space\n",
    "                and not token.pos_ == \"SYM\"\n",
    "                and not token.pos_ == \"DET\"\n",
    "                and not token.pos_ == \"X\"\n",
    "                #and not token.pos_ == \"SCONJ\"\n",
    "                #and not token.pos_ == \"CONJ\"\n",
    "                #and not token.pos_ == \"CCONJ\"\n",
    "                #and not token.pos_ == \"ADP\"\n",
    "                and not token.pos_ == \"NUM\"\n",
    "                #and not token.pos_ == \"AUX\"\n",
    "                and not token.pos_ == \"PRON\"\n",
    "                #and not token.pos_ == \"PROPN\"\n",
    "            ):\n",
    "                #texto lematizado y en minusculas\n",
    "                #newSentence = ' '.join([newSentence,token.lemma_.lower()])\n",
    "                #texto sin lematizar y en minusculas\n",
    "                newSentence = ' '.join([newSentence,token.text.lower()])\n",
    "                \n",
    "        tweets_clean.append(newSentence)\n",
    "    #print(tweets_clean)\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0e121f4-082b-4cc1-9280-64d36dda705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweetsTSV(corpus_path, trainingFile):\n",
    "    if (not os.path.exists(corpus_path)):\n",
    "        os.mkdir(corpus_path)\n",
    "\n",
    "    df_dev = pd.read_csv(trainingFile, sep=\"\\t\", usecols = ['id', 'tweet','emotion'])\n",
    "\n",
    "    anger_path = corpus_path+'//ANGER'\n",
    "    disgust_path = corpus_path+\"//DISGUST\"\n",
    "    fear_path = corpus_path+\"//FEAR\"\n",
    "    joy_path = corpus_path+\"//JOY\"\n",
    "    sadness_path = corpus_path + \"//SADNESS\"\n",
    "    surprise_path = corpus_path + \"//SURPRISE\"\n",
    "    others_path = corpus_path + \"//OTHERS\"\n",
    "\n",
    "    if (not os.path.exists(anger_path)):\n",
    "            os.makedirs(anger_path)\n",
    "            os.makedirs(disgust_path)\n",
    "            os.makedirs(fear_path)\n",
    "            os.makedirs(joy_path)\n",
    "            os.makedirs(sadness_path)\n",
    "            os.makedirs(surprise_path)\n",
    "            os.makedirs(others_path)\n",
    "\n",
    "    #print(df_dev)\n",
    "    df_dev.to_csv(\"filterDataTrain.csv\")\n",
    "\n",
    "    with open('filterDataTrain.csv', mode='r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file, quotechar='\"', delimiter=',',\n",
    "                                    quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "        path = ''\n",
    "        for row in csv_reader:\n",
    "            #print(row)\n",
    "            if row[\"emotion\"] == \"others\":\n",
    "                path= others_path\n",
    "            elif row[\"emotion\"] == \"anger\":\n",
    "                path= anger_path\n",
    "            elif row[\"emotion\"] == \"disgust\":\n",
    "                path= disgust_path\n",
    "            elif row[\"emotion\"] == \"fear\":\n",
    "                path= fear_path\n",
    "            elif row[\"emotion\"] == \"joy\":\n",
    "                path = joy_path\n",
    "            elif row[\"emotion\"] == \"sadness\":\n",
    "                path = sadness_path\n",
    "            elif row[\"emotion\"] == \"surprise\":\n",
    "                path = surprise_path\n",
    "\n",
    "            # Creating new file\n",
    "            f=open(path+\"//\"+row['id']+\".txt\",\"w\", encoding='utf-8')\n",
    "            f.write(row['tweet'])\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb4a40ad-1e22-4f39-b1fb-cb950d4974eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(corpus_path):\n",
    "\n",
    "    # CorpusTrain and subfolders (categories) must exist\n",
    "    tweets_data = load_files(corpus_path)\n",
    "    X, y = tweets_data.data, tweets_data.target\n",
    "    \n",
    "    #en esta funcion se hace un analisis exploratorio de los datos\n",
    "    analisis_tweets(X)\n",
    "    \n",
    "    #en esta funcion hacemos el procesado de datos y los limpiamos\n",
    "    documents = basic_processing(X)\n",
    "\n",
    "    #probamos las diferentes combinaciones de vectorizacion\n",
    "    \n",
    "    #1_BINARIO\n",
    "    #vectorizer = CountVectorizer(binary=True, ngram_range=(1,3))\n",
    "    #X = vectorizer.fit_transform(documents).toarray()\n",
    "    \n",
    "    \n",
    "    #2_TF-IDF\n",
    "    vectorizer = CountVectorizer()\n",
    "    #vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "    X = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "    tfidfconverter = TfidfTransformer()\n",
    "    X = tfidfconverter.fit_transform(X).toarray()\n",
    "    \n",
    "    \n",
    "    #3_TF\n",
    "    #vectorizer = CountVectorizer()\n",
    "    #vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "    #X = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "    # The data is divided into 20% test set and 80% training set.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    #probamos distintos algoritmos de clasificacion \n",
    "    #clf = MultinomialNB().fit(X_train, y_train)\n",
    "    #clf = BernoulliNB().fit(X_train, y_train)\n",
    "    #clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "    #clf = ExtraTreeClassifier().fit(X_train, y_train)\n",
    "    clf = LinearSVC().fit(X_train, y_train)\n",
    "    #clf = KNeighborsClassifier().fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"------------------------------------------\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"accuracy\",accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54bc55ca-5c24-4557-94a1-f5e983f1ffc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity categories:  Counter({'MISC': 454, 'LOC': 413, 'PER': 318, 'ORG': 152})\n",
      "*********************************\n",
      "Tipo de palabras mas frecuentes:  ['PROPN', 'NUM', 'CCONJ', 'ADV', 'ADJ', 'AUX', 'PRON', 'VERB', 'PUNCT', 'NOUN', 'ADP']\n",
      "*********************************\n",
      "*********************************\n",
      "Palabras mas frecuentes:  ['barcelona', '️', 'venezuela', 'libro', 'notredame', 'campeón', 'incendio', '|', 'mundo', 'liverpool', 'historia', 'capítulo', 'laliga', 'gente', 'y', 'elecccionesgenerales28a', 'años', 'messi', '🏆', 'gracias', '⚽', 'o', 'díadellibro', 'user', 'juegodetronos', 'gretathunberg', 'diadellibro', 'championsleague', 'españa', 'a', 'gameofthrones', 'libros']\n",
      "*********************************\n",
      "[[  2   0   0   4  22   1   2]\n",
      " [  1   0   0   1   5   0   0]\n",
      " [  0   0   2   0   3   0   0]\n",
      " [  1   0   0  26  31   1   0]\n",
      " [  0   0   0  12 116   2   1]\n",
      " [  1   0   0   0  10  11   0]\n",
      " [  0   0   0   1   6   0   1]]\n",
      "------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.06      0.11        31\n",
      "           1       0.00      0.00      0.00         7\n",
      "           2       1.00      0.40      0.57         5\n",
      "           3       0.59      0.44      0.50        59\n",
      "           4       0.60      0.89      0.72       131\n",
      "           5       0.73      0.50      0.59        22\n",
      "           6       0.25      0.12      0.17         8\n",
      "\n",
      "    accuracy                           0.60       263\n",
      "   macro avg       0.51      0.35      0.38       263\n",
      "weighted avg       0.57      0.60      0.55       263\n",
      "\n",
      "------------------------------------------\n",
      "accuracy 0.6007604562737643\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    trainingFile = \"Corpus/train.tsv\"\n",
    "    corpus_path = \"CorpusTraining\"\n",
    "\n",
    "    \n",
    "    processTweetsTSV(corpus_path, trainingFile)\n",
    "    # entrenar el clasificador y evaluar el rendimiento\n",
    "    classifier(corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75483c8d-3ffb-4e89-9c7c-4460d4bcb338",
   "metadata": {},
   "source": [
    "# PRUEBAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13c34273-dd56-400d-a8f9-21ec7e5cd2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"Hi \\xF0\\x9F\\x98\\x81 How is your 🙈 and 😌. Have a nice weekend 💕👭👙\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32ee6f8b-1c8c-43df-bc0d-7e103a12b5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi ð How is your  and . Have a nice weekend \n"
     ]
    }
   ],
   "source": [
    "def give_emoji_free_text(text):\n",
    "    return emoji.get_emoji_regexp().sub(u'', text)\n",
    "\n",
    "print (give_emoji_free_text(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c450327-e9a2-423a-ace4-44a8500893a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#s2 = \" b'os presentar a santa arya stark . \\\\xf0\\\\x9f\\\\x99\\\\x8f\"\n",
    "s2 = b'USER Que mal eres chavista? ! Por pensar as\\xc3\\xad no vas a salir de pobre ajajaj. #VenezuelaLibre #MaduroNoEsNadie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "809c72af-ef23-4c8a-8545-2ee32e7ad702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b'os presentar a santa arya stark . ð\n"
     ]
    }
   ],
   "source": [
    "def give_emoji_free_text(text):\n",
    " #   return emoji.get_emoji_regexp().sub(u'', text)\n",
    "    emoji_pattern = re.compile(pattern = \"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "print (give_emoji_free_text(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8edf512-3113-44d8-9beb-6c47618c68db",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-22be26f4284c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ascii'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "text = s2.encode('ascii', 'ignore').decode()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ae2d5db-fed2-487b-9f17-ed9d4bb70edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b'os presentar a santa arya stark . \\xf0\\x9f\\x99\\x8f\n"
     ]
    }
   ],
   "source": [
    "text2 = text.encode('ascii', 'ignore').decode()\n",
    "\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f952e7fb-0ed6-470e-959d-4779e553d10e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practica_text_mining_parte1",
   "language": "python",
   "name": "practica_text_mining_parte1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
