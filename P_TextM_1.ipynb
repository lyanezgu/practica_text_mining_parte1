{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7933fd05-298c-4988-810a-cfd669e5f48a",
   "metadata": {},
   "source": [
    "# * *PRÁCTICA FINAL SOTO ANÁLISIS DE SENTIMIENTOS* *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d681084-27d6-481c-800a-b883fb4a60aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import spacymoji\n",
    "from spacymoji import Emoji\n",
    "\n",
    "sys.path.append(os.path.realpath('../'))\n",
    "# from scripts.preprocess_data import Preprocess\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PATH_DATA = \"../\"\n",
    "DEV_FILE = \"dev/dev.tsv\"\n",
    "TRAIN_FILE = \"Corpus/train.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ddd27ca-cc4b-4f28-848a-839608b4741e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_emoji_free_text(text):\n",
    "    #return emoji.get_emoji_regexp().sub(u'', text) # requiere que no sea string\n",
    "    text = text.encode('ascii', 'ignore') #.decode()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e9431bf-6b7f-4b3d-92d0-34a15c6c5320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_processing(X):\n",
    "    \n",
    "\n",
    "    tweets_clean = []\n",
    "    \n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "    for pos, sen in enumerate(X):\n",
    "        #print(sen)\n",
    "        tweet = str(sen)\n",
    "        print(tweet)\n",
    "        tweet = give_emoji_free_text(sen)\n",
    "        #tweet = str(tweet)\n",
    "        #tweet = str(X[sen])\n",
    "        tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove tweeted at (menciones)\n",
    "        #tweet = re.sub('(#\\s+[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove hashtags at (hashtag)\n",
    "        tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet)  # remove hashtags + espacio at (hashtag con espacio)\n",
    "        doc = nlp(tweet)\n",
    "\n",
    "            #obtenemos la frase lematizada \n",
    "        lista_tweet=[]\n",
    "        newSentence=''\n",
    "        for token in doc:\n",
    "            if (\n",
    "                #not token.is_punct\n",
    "                #not token.is_emoji\n",
    "                #and not token.like_num\n",
    "                not token.is_stop\n",
    "                and not token.like_url\n",
    "                and not token.is_space\n",
    "                and not token.pos_ == \"SYM\"\n",
    "                #and not token.pos_ == \"DET\"\n",
    "                and not token.pos_ == \"X\"\n",
    "                #and not token.pos_ == \"SCONJ\"\n",
    "                #and not token.pos_ == \"CONJ\"\n",
    "                #and not token.pos_ == \"ADP\"\n",
    "                #and not token.pos_ == \"INTJ\"\n",
    "                and not token.pos_ == \"NUM\"\n",
    "                #and not token.pos_ == \"PRON\"\n",
    "                #and not token.pos_ == \"PROPN\"\n",
    "            ):\n",
    "                newSentence = ' '.join([newSentence,token.lemma_.lower()])\n",
    "                #lista_tweet.append(newSentence)\n",
    "        tweets_clean.append(newSentence)\n",
    "    print(tweets_clean)\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0e121f4-082b-4cc1-9280-64d36dda705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTweetsTSV(corpus_path, trainingFile):\n",
    "    if (not os.path.exists(corpus_path)):\n",
    "        os.mkdir(corpus_path)\n",
    "\n",
    "    df_dev = pd.read_csv(trainingFile, sep=\"\\t\", usecols = ['id', 'tweet','emotion'])\n",
    "\n",
    "    anger_path = corpus_path+'\\\\ANGER'\n",
    "    disgust_path = corpus_path+\"\\\\DISGUST\"\n",
    "    fear_path = corpus_path+\"\\\\FEAR\"\n",
    "    joy_path = corpus_path+\"\\\\JOY\"\n",
    "    sadness_path = corpus_path + \"\\\\SADNESS\"\n",
    "    surprise_path = corpus_path + \"\\\\SURPRISE\"\n",
    "    others_path = corpus_path + \"\\\\OTHERS\"\n",
    "\n",
    "    if (not os.path.exists(anger_path)):\n",
    "            os.makedirs(anger_path)\n",
    "            os.makedirs(disgust_path)\n",
    "            os.makedirs(fear_path)\n",
    "            os.makedirs(joy_path)\n",
    "            os.makedirs(sadness_path)\n",
    "            os.makedirs(surprise_path)\n",
    "            os.makedirs(others_path)\n",
    "\n",
    "    #print(df_dev)\n",
    "    df_dev.to_csv(\"filterDataTrain.csv\")\n",
    "\n",
    "    with open('filterDataTrain.csv', mode='r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file, quotechar='\"', delimiter=',',\n",
    "                                    quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "        path = ''\n",
    "        for row in csv_reader:\n",
    "            #print(row)\n",
    "            if row[\"emotion\"] == \"others\":\n",
    "                path= others_path\n",
    "            elif row[\"emotion\"] == \"anger\":\n",
    "                path= anger_path\n",
    "            elif row[\"emotion\"] == \"disgust\":\n",
    "                path= disgust_path\n",
    "            elif row[\"emotion\"] == \"fear\":\n",
    "                path= fear_path\n",
    "            elif row[\"emotion\"] == \"joy\":\n",
    "                path = joy_path\n",
    "            elif row[\"emotion\"] == \"sadness\":\n",
    "                path = sadness_path\n",
    "            elif row[\"emotion\"] == \"surprise\":\n",
    "                path = surprise_path\n",
    "\n",
    "            # Creating new file\n",
    "            f=open(path+\"\\\\\"+row['id']+\".txt\",\"w\", encoding='utf-8')\n",
    "            f.write(row['tweet'])\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb4a40ad-1e22-4f39-b1fb-cb950d4974eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(corpus_path):\n",
    "\n",
    "    # CorpusTrain and subfolders (categories) must exist\n",
    "    tweets_data = load_files(corpus_path)\n",
    "    X, y = tweets_data.data, tweets_data.target\n",
    "\n",
    "    documents = basic_processing(X)\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    #vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, ngram_range=(1, 1))\n",
    "    X = vectorizer.fit_transform(documents).toarray()\n",
    "\n",
    "    tfidfconverter = TfidfTransformer()\n",
    "    X = tfidfconverter.fit_transform(X).toarray()\n",
    "\n",
    "    # The data is divided into 20% test set and 80% training set.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    #clf = MultinomialNB().fit(X_train, y_train)\n",
    "    #clf = BernoulliNB().fit(X_train, y_train)\n",
    "    #clf = GaussianNB().fit(X_train, y_train)\n",
    "    #clf = ComplementNB().fit(X_train, y_train)\n",
    "    #clf = CategoricalNB().fit(X_train, y_train)\n",
    "    #clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "    #clf = ExtraTreeClassifier().fit(X_train, y_train)\n",
    "    clf = LinearSVC().fit(X_train, y_train)\n",
    "    #clf = SVC().fit(X_train, y_train)\n",
    "    #clf = KNeighborsClassifier().fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "    print(\"------------------------------------------\")\n",
    "    print(classification_report(y_test,y_pred))\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"accuracy\",accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54bc55ca-5c24-4557-94a1-f5e983f1ffc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'USER Que mal eres chavista? ! Por pensar as\\xc3\\xad no vas a salir de pobre ajajaj. #VenezuelaLibre #MaduroNoEsNadie'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-442fbb3cffe3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprocessTweetsTSV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainingFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# entrenar el clasificador y evaluar el rendimiento\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-4b118e861912>\u001b[0m in \u001b[0;36mclassifier\u001b[1;34m(corpus_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweets_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtweets_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbasic_processing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-663f791d290d>\u001b[0m in \u001b[0;36mbasic_processing\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgive_emoji_free_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;31m#tweet = str(tweet)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m#tweet = str(X[sen])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-1f5808a17f64>\u001b[0m in \u001b[0;36mgive_emoji_free_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgive_emoji_free_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#return emoji.get_emoji_regexp().sub(u'', text) # requiere que no sea string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ascii'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#.decode()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    trainingFile = \"Corpus/train.tsv\"\n",
    "    corpus_path = \"CorpusTraining\"\n",
    "\n",
    "    processTweetsTSV(corpus_path, trainingFile)\n",
    "    # entrenar el clasificador y evaluar el rendimiento\n",
    "    classifier(corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75483c8d-3ffb-4e89-9c7c-4460d4bcb338",
   "metadata": {},
   "source": [
    "# PRUEBAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13c34273-dd56-400d-a8f9-21ec7e5cd2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"Hi \\xF0\\x9F\\x98\\x81 How is your 🙈 and 😌. Have a nice weekend 💕👭👙\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32ee6f8b-1c8c-43df-bc0d-7e103a12b5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi ð How is your  and . Have a nice weekend \n"
     ]
    }
   ],
   "source": [
    "def give_emoji_free_text(text):\n",
    "    return emoji.get_emoji_regexp().sub(u'', text)\n",
    "\n",
    "print (give_emoji_free_text(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c450327-e9a2-423a-ace4-44a8500893a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#s2 = \" b'os presentar a santa arya stark . \\\\xf0\\\\x9f\\\\x99\\\\x8f\"\n",
    "s2 = b'USER Que mal eres chavista? ! Por pensar as\\xc3\\xad no vas a salir de pobre ajajaj. #VenezuelaLibre #MaduroNoEsNadie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "809c72af-ef23-4c8a-8545-2ee32e7ad702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b'os presentar a santa arya stark . ð\n"
     ]
    }
   ],
   "source": [
    "def give_emoji_free_text(text):\n",
    " #   return emoji.get_emoji_regexp().sub(u'', text)\n",
    "    emoji_pattern = re.compile(pattern = \"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "print (give_emoji_free_text(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8edf512-3113-44d8-9beb-6c47618c68db",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-22be26f4284c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ascii'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "text = s2.encode('ascii', 'ignore').decode()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ae2d5db-fed2-487b-9f17-ed9d4bb70edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " b'os presentar a santa arya stark . \\xf0\\x9f\\x99\\x8f\n"
     ]
    }
   ],
   "source": [
    "text2 = text.encode('ascii', 'ignore').decode()\n",
    "\n",
    "print(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f952e7fb-0ed6-470e-959d-4779e553d10e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
